[
  {
    "path": "posts/2021-10-20-data-retrieval/",
    "title": "Using an API to Programmatically Access and Download Data",
    "description": "Using the `dataRetrieval` package to analyze USGS datasets",
    "author": [
      {
        "name": "Felicia Cruz",
        "url": {}
      }
    ],
    "date": "2021-10-20",
    "categories": [
      "packages",
      "MEDS"
    ],
    "contents": "\n\nContents\nTime Series for 2021\nMontecito Mudslides: January 9, 2018\n\nDuring my EDS 213 course titled “Metadata Standards, Data Modeling and Data Semantics” I learned how to use an API to programmatically retrieve data.\nUsing the dataRetrieval package to access USGS data water gauge data, I will be looking at the Ventura River and Santa Paula Creek gauges.\nTime Series for 2021\n\n\nShow code\n\n# Query both sites at once by passing in a vector for the siteNumber argument  \n# Ventura is 1111850\n# Santa Paula is 1111350\n\ncombined <- readNWISdv(siteNumber = c(\"11118500\", \"11113500\"),\n                       parameterCd = \"00060\",\n                       startDate = \"2021-01-01\",\n                       endDate = \"2021-10-21\")\n\n# rename the discharge column and drop the unnecessary variables \n\ncombined$discharge <- combined$X_00060_00003\ncombined <- combined %>% \n  select(site_no, Date, discharge) %>% \n  mutate(site_no = case_when(site_no == \"11118500\" ~ \"Ventura\",\n            TRUE ~ \"Santa Paula\"))\n\n\n\nOne of my favorite things about living in Santa Barbara is the beautiful weather. With sunny skies and warm temperatures most days, I have become quite sensitive to even small drops in temperature or brief rain episodes. In 2021, we have experienced very little rain thus far. From the plot below, we can see that the highest level of water discharge for either site was less than 15 cubic feet per second. Besides this spike, levels were consistently under 5 cubic feet per second.\n\n\nShow code\n\n# make a time series plot with both sites \n\n# subset for just 2021 \n\nggplot(data = combined, aes(x = Date, y = discharge)) +\n  geom_line(aes(color = site_no)) +\n  ylim(0, 15) +\n  labs(title = \"Water Discharge, Santa Paula and Ventura\",\n       subtitle = \"2021-01-01 - 2021-10-21\",\n       color = \"Site\",\n       y = \"Discharge (cubic ft/s)\")\n\n\n\n\nMontecito Mudslides: January 9, 2018\nThe Thomas Fire in December of 2018 ended my first quarter of college early and postponed my first finals week. Once the fire was contained after the holidays, I thought I would be returning to a somewhat normal college experience…until the Montecito Mudslides happened. After a quick weekend trip home in January, the 101 closure due to these mudslides meant in order to get back to school I needed to take a short boat trip from Ventura Harbor to Santa Barbara.\nJust as the Thomas Fire was nearing complete containment, heavy rainfall on January 9 washed away massive amounts of mud and debris loosened from the areas burned by the fire. These mudslides sadly resulted in 23 deaths and the destruction of around 100 residences.\nBy using the readNWISuv function, I can read in the water discharge data from the USGS for January 9, 2018 to see when rainfall was most intense.\n\n\nShow code\n\n# getting instantaneous data for January 9 to look at water discharge \n\ncombined_jan_9_18 <- readNWISuv(siteNumbers =  c(\"11118500\", \"11113500\"),\n                       parameterCd = \"00060\",\n                       startDate = \"2018-01-09\",\n                       endDate = \"2018-01-09\",\n                       tz = \"America/Los_Angeles\")\n\ncombined_jan_9_18$discharge <- combined_jan_9_18$X_00060_00000\n\ncombined_jan_9_18 <- combined_jan_9_18 %>% \n  select(site_no, dateTime, discharge) %>% \n  mutate(site_no = case_when(site_no == \"11118500\" ~ \"Ventura\",\n            TRUE ~ \"Santa Paula\"))\n\nggplot(combined_jan_9_18, aes(x = dateTime, y = discharge)) +\n  geom_line(aes(color = site_no)) +\n  labs(y = \"Discharge (cubic ft/s)\",\n       color = \"Site\",\n       title = \"Water Discharge\",\n       subtitle = \"2018-01-09\",\n       x = \"Time\") \n\n\n\n\nFrom the above plot, we can see that water discharge peaked at around 6000 cubic feet per second at about 7 am. Compared to the maximum discharge in 2021 so far of 15 cubic feet per second, this highlights the intensity of the rainfall that triggered the Montecito Mudslides in January of 2018. In a place of such low rainfall like Santa Barbara County, it is unlikely that such a massive rain event would occur and bring with it such devastating consequences, but the burn scars left by the Thomas Fire made these mudslides practically inevitable.\n\n\n\n",
    "preview": "posts/2021-10-20-data-retrieval/images/dataretrieval_hex.png.crdownload",
    "last_modified": "2021-11-01T14:58:19-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-16-aug-16/",
    "title": "Two weeks as a MEDS student",
    "description": "A few functions I learned and key takeaways from my first two weeks in MEDS",
    "author": [
      {
        "name": "Felicia Cruz",
        "url": {}
      }
    ],
    "date": "2021-08-16",
    "categories": [
      "packages",
      "MEDS"
    ],
    "contents": "\nSome functions I have found particularly useful are group_by() and summarize(). Working together, these two functions will recognize groups that are specified in the arguments and then compute chosen summary statistics represented in a new data frame.\nHere is an example of how these functions can be used to find the mean flipper length for each penguin species in the palmerpenguins dataset.\n\n\nflipper_length_by_species <- penguins %>%\n  group_by(species) %>% \n  summarize(mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE))\n\n\n\n\n# A tibble: 3 × 2\n  species   mean_flipper_length\n  <fct>                   <dbl>\n1 Adelie                   190.\n2 Chinstrap                196.\n3 Gentoo                   217.\n\nJust by using these two functions, from the new data frame we can quickly see that Gentoos have the longest flipper length on average.\nBeyond just learning useful functions, I have also deepened my understanding of tidy data. While I used to think that “tidy data” simply referred to datasets that were visually clean and organized, I have come to learn that tidy data refers to data that follows a certain structure. The three criteria that must be met for data to be considered “tidy” are:\nEach variable must be a column\nEach observation must be a row\nEach cell can contain only one value\nWhen data is in tidy form, this allows for easier visualizations and analyses.\nLastly, a key takeaway I have learned in my first two weeks as a data science student is that collaboration is critical. Whether collaboration means testing functions together or simply spotting an unmatched parentheses, I have really enjoyed the teammwork aspect that comes along with working in this field.\n\n\n\n",
    "preview": "posts/2021-08-16-aug-16/images/bren_outside.jpeg",
    "last_modified": "2021-11-01T14:28:12-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-10-a-function-i-learned/",
    "title": "Working with ggplot",
    "description": "using ggplot with the palmerpenguins package",
    "author": [
      {
        "name": "Felicia Cruz",
        "url": {}
      }
    ],
    "date": "2021-08-10",
    "categories": [],
    "contents": "\n\n\n\nThis scatterplot shows a positive correlation between penguin flipper length and body mass. While the Adelie and Chinstrap species are similar in size, the Gentoos weigh a lot more and have much longer flippers.\n\n\n\n",
    "preview": "posts/2021-08-10-a-function-i-learned/images/palmerpenguins.jpg",
    "last_modified": "2021-11-01T14:28:12-07:00",
    "input_file": {}
  }
]
